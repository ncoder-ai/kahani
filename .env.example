# Kahani Environment Configuration
# Copy this file to .env and customize the values

# ===================
# Application Ports
# ===================
BACKEND_PORT=9876
FRONTEND_PORT=6789

# ===================
# Database
# ===================
DATABASE_URL=sqlite:///./data/kahani.db

# ===================
# Security
# ===================
SECRET_KEY=change-this-secret-key-in-production
JWT_SECRET_KEY=change-this-jwt-key-in-production
JWT_ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=30

# ===================
# Admin User
# ===================
ADMIN_EMAIL=admin@localhost
ADMIN_PASSWORD=changeme123

# ===================
# LLM Configuration
# ===================
# LM Studio (default - local)
LLM_BASE_URL=http://localhost:1234/v1
LLM_API_KEY=not-needed
LLM_MODEL=local-model
LLM_MAX_TOKENS=2048
LLM_TEMPERATURE=0.7

# Ollama (alternative local)
# LLM_BASE_URL=http://localhost:11434/v1
# LLM_API_KEY=not-needed
# LLM_MODEL=llama2

# OpenAI (cloud)
# LLM_BASE_URL=https://api.openai.com/v1
# LLM_API_KEY=sk-your-api-key-here
# LLM_MODEL=gpt-3.5-turbo

# ===================
# TTS (Optional)
# ===================
TTS_PROVIDER=
TTS_API_URL=
TTS_API_KEY=
TTS_VOICE=

# OpenAI TTS example:
# TTS_PROVIDER=openai
# TTS_API_URL=https://api.openai.com/v1
# TTS_API_KEY=sk-your-api-key
# TTS_VOICE=alloy

# ===================
# Frontend
# ===================
NEXT_PUBLIC_API_BASE_URL=http://localhost:9876

# ===================
# CORS
# ===================
# CORS - Cross-Origin Resource Sharing
# Allow requests from these origins (JSON array format required)
# For development, include all localhost ports your frontend might use
CORS_ORIGINS=["http://localhost:6789","http://localhost:3001","http://127.0.0.1:6789","http://0.0.0.0:6789"]
# For local network access (mobile devices), you can use:
# CORS_ORIGINS=["*"]
# Note: Using ["*"] allows all origins - only use in development!

# ===================
# Application
# ===================
APP_NAME=Kahani
DEBUG=false

# ===================
# Context Management
# ===================
# Token buffer for safety margin (0.1-1.0, default: 0.9 = 90%)
# Uses 90% of max tokens to prevent LLM errors from token counting variations
CONTEXT_TOKEN_BUFFER=0.9
